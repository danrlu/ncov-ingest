#!/usr/bin/env python3
"""
Parse the GenBank JSON load into a metadata tsv and a FASTA file.
"""
import os
import argparse
import csv
import sys
from pathlib import Path
import re
import pandas as pd
from typing import List

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from utils.transform import (
    METADATA_COLUMNS,
    standardize_dataframe,
    fill_default_geo_metadata,
    write_fasta_file,

)
from utils.transformpipeline import LINE_NUMBER_KEY
from utils.transformpipeline.datasource import LineToJsonDataSource
from utils.transformpipeline.filters import SequenceLengthFilter, LineNumberFilter, GenbankProblematicFilter
from utils.transformpipeline.transforms import (
    AbbreviateAuthors,
    ApplyUserGeoLocationSubstitutionRules,
    AddHardcodedMetadataGenbank,
    DropSequenceData,
    FillDefaultLocationData,
    MergeUserAnnotatedMetadata,
    ParseGeographicColumnsGenbank,
    RenameAndAddColumns,
    StandardizeData,
    StandardizeGenbankStrainNames,
    Tracker,
    UserProvidedAnnotations,
    UserProvidedGeoLocationSubstitutionRules,
)

assert 'sequence' not in METADATA_COLUMNS, "Sequences should not appear in metadata!"

def preprocess(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Rename and standardize column types and drop records where the
    sequence length is less than 15kkb.
    Returns the modified DataFrame.
    """
    mapper = {
        'collected': 'date',
        'submitted': 'date_submitted',
    }

    # Standardize column names, dtypes and drop entries with length less than 15kb
    return standardize_dataframe(genbank_data, mapper)


def standardize_strain_names(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Attempt to standardize strain names by removing extra prefixes,
    stripping spaces, and correcting known common error patterns.
    """
    # Compile list of regex to be used for strain name standardization
    # Order is important here! Keep the known prefixes first!
    regex_replacement = [
        (r'(^SAR[S]{0,1}[-\s]CoV[-]{0,1}2/|^2019[-\s]nCoV[-_\s/]|^BetaCoV/|^nCoV-|^hCoV-19/)',''),
        (r'(human/|homo sapien/|Homosapiens/)',''),
        (r'^USA-', 'USA/'),
        (r'^USACT-', 'USA/CT-'),
        (r'^USAWA-', 'USA/WA-'),
        (r'^HKG.', 'HongKong/'),
    ]

    # Parse strain name from title to fill in strains that are empty strings
    genbank_data['strain_from_title'] = genbank_data['title'].apply(parse_strain_from_title)
    genbank_data.loc[(genbank_data['strain'] == ''), 'strain'] = genbank_data['strain_from_title']

    # Standardize strain names using list of regex replacements
    for regex, replacement in regex_replacement:
        genbank_data['strain'] = genbank_data['strain'] \
            .str.replace(regex, replacement, n=1, case=False)

    # Strip all spaces
    genbank_data['strain'] = genbank_data['strain'].str.replace(r'\s', '')

    return genbank_data


def parse_strain_from_title(title: str) -> str:
    """
    Try to parse strain name from the given *title* using regex search.
    Returns an empty string if not match is found in the *title*.
    """
    strain_name_regex = r'[-\w]*/[-\w]*/[-\w]*\s'
    strain = re.search(strain_name_regex, title)
    return strain.group(0) if strain else ''


def parse_geographic_columns(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Expands string found in the column named `location` in the given
    *genbank_data* DataFrame, creating 3 new columns. Returns the modified
    DataFrame.

    Expected formats of the location string are:
        * "country"
        * "country: division"
        * "country: division, location"
    """
    # Create dict of US state codes and their full names
    us_states = pd.read_csv(base / 'source-data/us-state-codes.tsv', header=None, sep='\t', comment="#")
    us_states = dict(zip(us_states[0], us_states[1]))

    geographic_data = genbank_data['location'].str.split(':\s*', expand=True)
    geographic_data[0] = geographic_data[0].str.strip()

    divisions = []
    locations = []
    i =0
    for index, value in geographic_data.iterrows():

        if len(value)<2:# Both location and division are not available - special case where no records in the source json have this
            divisions.append(None)
            locations.append(None)
            continue

        # Both location and division are not available
        if pd.isna(value[1]):
            location = division = None
        # Only division is available
        elif ',' not in value[1]:
            location = None
            division = value[1]
        # Location and division are both available
        elif ',' in value[1]:
            division,location = value[1].split(',', 1)
        # Unknown format of location data
        else:
            assert False, f"Found unknown format for geographic data: {value}"

        # Special parsing for US locations because the format varies
        if value[0] == 'USA' and division:
            # Switch location & division if location is a US state
            if location and any(location.strip() in s for s in us_states.items()):
                state = location
                location = division
                division = state
            # Convert US state codes to full names
            if us_states.get(division.strip().upper()):
                division = us_states[division.strip().upper()]


        location = location.strip().lower().title() if location else None
        division = division.strip().lower().title() if division else None

        divisions.append(division)
        locations.append(location)


    genbank_data['country']     = geographic_data[0]
    genbank_data['division']    = pd.Series(divisions)
    genbank_data['location']    = pd.Series(locations)

    return genbank_data


def parse_authors(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Abbreviate the column named `authors` to be "<first author> etl al" rather than a
    full list. Returns the modified DataFrame.
    """
    # Strip and normalize whitespace
    genbank_data['authors'] = genbank_data['authors'].str.replace(r'\s+', ' ')
    # Strip to string before first comma
    genbank_data['authors'] = genbank_data['authors'].str.replace(r"^([^,]+),.+", lambda m: m.group(1))
    # Add et al to authors if authors is not an empty string
    genbank_data.loc[genbank_data['authors'] != '', 'authors'] = genbank_data['authors'].astype(str) + ' et al'
    # Replace blank authors with '?'
    genbank_data.loc[genbank_data['authors'] == '', 'authors'] = '?'

    return genbank_data


def generate_hardcoded_metadata(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Returns a DataFrame with a column for GenBank accession plus
    additional columns containing hardcoded metadata.
    """
    hardcoded_metadata = pd.DataFrame(genbank_data['genbank_accession'])
    hardcoded_metadata['virus']             = 'ncov'
    hardcoded_metadata['gisaid_epi_isl']    = '?'
    hardcoded_metadata['segment']           = 'genome'
    hardcoded_metadata['age']               = '?'
    hardcoded_metadata['sex']               = '?'
    hardcoded_metadata['pango_lineage']  = '?'
    hardcoded_metadata['GISAID_clade']      = '?'
    hardcoded_metadata['originating_lab']   = '?'
    hardcoded_metadata['submitting_lab']    = '?'
    hardcoded_metadata['paper_url']         = '?'
    hardcoded_metadata['sampling_strategy']         = '?'

    hardcoded_metadata['url'] = hardcoded_metadata['genbank_accession'] \
        .apply(lambda x: f"https://www.ncbi.nlm.nih.gov/nuccore/{x}")

    return hardcoded_metadata


def apply_geoRules( genbank_data: pd.DataFrame , rulesFile : str) -> pd.DataFrame:

    geoRules = UserProvidedGeoLocationSubstitutionRules()
    if rulesFile :
        # use curated rules to subtitute known spurious locations with correct ones
        with open(rulesFile,'r') as geo_location_rules_fh :
            for line in geo_location_rules_fh:
                geoRules.readFromLine( line )



        def replaceInList(l,a,b):
            for i in range(len(l)):
                if l[i]==a:
                    l[i]=b
            return l

        for index, row in genbank_data.iterrows():

            LOCATION_COLUMNS = ['region', 'country', 'division', 'location']
            locs = [ row[col] for col in LOCATION_COLUMNS ]
            locs = replaceInList(locs,None,'') ## expected empty for rules is ''
            newVal = geoRules.get_user_rules( tuple( locs ) )
            newVal = tuple( replaceInList(list(newVal),'',None) ) ## re-putting the default expected by the rest of the code
            for i,key in enumerate(LOCATION_COLUMNS):
                row[key] = newVal[i]
    
    return genbank_data

def update_metadata(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Update metadata with hardcoded metadata and update with curated
    annotations if provided. Returns the modified DataFrame.
    """
    hardcoded_metadata = generate_hardcoded_metadata(genbank_data)
    genbank_data = genbank_data.merge(hardcoded_metadata)

    if args.annotations:
        # Use the curated annotations tsv to update any column values
        user_provided_annotations = pd.read_csv(args.annotations, header=None, sep='\t', comment="#")
        for index, (genbank_accession, key, value) in user_provided_annotations.iterrows():
            # Strip any whitespace remaining from inline comments after the final column.
            if isinstance(value, str):
                value = value.rstrip()
            genbank_data.loc[genbank_data['genbank_accession'] == genbank_accession, key] = value

    if args.accessions:
        with open(args.accessions, "r") as accessions_fh:
            for row in csv.DictReader(accessions_fh, delimiter='\t'):
                # Our GenBank metadata file is currently indexed on the
                # non-versioned accession.
                genbank_data.loc[genbank_data["genbank_accession_rev"] == row["genbank_accession"], "gisaid_epi_isl"] = row["gisaid_epi_isl"]

    # Fill in blank geographic columns with exisiting geographic data
    genbank_data = fill_default_geo_metadata(genbank_data)

    return genbank_data




def find_and_drop_problem_records(genbank_data: pd.DataFrame) -> pd.DataFrame:
    """
    Find records that are missing geographic regions or have the wrong
    name structure and print them out for manual curation. Drop the problem
    records and duplicate records and return the modified DataFrame.
    """
    strain_name_regex = r'([\w]*/)?[\w]*/[-_\.\w]*/[\d]{4}'

    problem_data = genbank_data.loc[(genbank_data.region == '') \
        | (genbank_data.country == '') \
        # All strain names should have structure {}/{}/{year} or {}/{}/{}/{year}
        # with the exception of 'Wuhan-Hu-1/2019'
        | (~(genbank_data.strain.str.match(strain_name_regex)) & (genbank_data.strain != 'Wuhan-Hu-1/2019'))]

    # Print problem records for manual curation
    if not problem_data.empty:
        problem_data[['genbank_accession', 'strain', 'region', 'country', 'url']].to_csv(args.problem_data, sep='\t', index=False)

    # Drop entries without geographic region or with wrong strain name structure
    genbank_data.drop(problem_data.index, inplace=True)

    # Drop duplicates, prefer longest and earliest sequence
    # based on submission date.
    genbank_data.sort_values(['strain', 'length', 'date_submitted'],
        ascending=[True, False, True], inplace=True)
    genbank_data.drop_duplicates('strain', inplace=True)

    return genbank_data


if __name__ == '__main__':
    base = Path(__file__).resolve().parent.parent

    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("genbank_data",
        default="s3://nextstrain-data/files/ncov/open/genbank.ndjson.gz",
        nargs="?",
        help="Newline-delimited GenBank JSON data")
    parser.add_argument("--annotations",
        default=base / "source-data/genbank_annotations.tsv",
        help="Optional manually curated annotations TSV.\n"
            "The TSV file should have no header and exactly three columns which contain:\n\t"
            "1. the GenBank accession number\n\t"
            "2. the column name to replace from the generated `metadata.tsv` file\n\t"
            "3. the replacement data\n"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "MT039888	location    Boston\n\t"
        "# First Californian sample\n\t"
        "MN994467	country_exposure	China\n\t"
        "MN908947	collection_date 2019-12-26 # Manually corrected date")
    parser.add_argument("--accessions",
        default=base / "source-data/accessions.tsv",
        help="Optional manually curated TSV cross-referencing accessions between databases (e.g. GISAID and GenBank/INSDC).")
    parser.add_argument("--output-metadata",
        default=base / "data/genbank/metadata.tsv",
        help="Output location of generated metadata tsv. Defaults to `data/genbank/metadata.tsv`")
    parser.add_argument("--output-fasta",
        default=base / "data/genbank/sequences.fasta",
        help="Output location of generated FASTA file. Defaults to `data/genbank/sequences.fasta`")
    parser.add_argument("--problem-data",
        default=base / "data/genbank/problem_data.tsv",
        help="Output location of generated tsv of problem records missing geography region or a valid strain name")
    parser.add_argument("--sorted-fasta", action="store_true",
        help="Sort the fasta file in the same order as the metadata file.  WARNING: Enabling this option can consume a lot of memory.")
    parser.add_argument("--geo-location-rules",
        default = str( base / "source-data/gisaid_geoLocationRules.tsv" ) ,
        help="Optional manually curated rules to correct geographical location.\n"
            "The TSV file should have no header and exactly 2 columns in the following format:\n\t"
            "region/country/division/location<tab>region/country/division/location"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "Europe/Spain/Catalunya/Mataró\tEurope/Spain/Catalunya/Mataro\n\t")
    parser.add_argument(
        "--output-unix-newline",
        dest="newline",
        action="store_const",
        const="\n",
        default=os.linesep,
        help="When specified, always use unix newlines in output files."
    )
    args = parser.parse_args()


    #parsing curated annotations
    annotations = UserProvidedAnnotations()
    if args.annotations:
        # Use the curated annotations tsv to update any column values
        with open(args.annotations, "r") as gisaid_fh:
            csvreader = csv.reader(gisaid_fh, delimiter='\t')
            for row in csvreader:
                if row[0].lstrip()[0] == '#':
                    continue
                elif len(row) != 3:
                    print("WARNING: couldn't decode annotation line " + "\t".join(row))
                    continue
                strainId, key, value = row
                annotations.add_user_annotation(
                    strainId,
                    key,
                    # remove the comment and the extra ws from the value
                    value.split('#')[0].rstrip(),
                )


    accessions = UserProvidedAnnotations()
    if args.accessions:
        with open(args.accessions, "r") as accessions_fh:
            for row in csv.DictReader(accessions_fh, delimiter='\t'):
                accessions.add_user_annotation(
                    row["genbank_accession"],
                    "gisaid_epi_isl",
                    row["gisaid_epi_isl"],
                )
    
    geoRules = UserProvidedGeoLocationSubstitutionRules()
    if args.geo_location_rules :
        # use curated rules to subtitute known spurious locations with correct ones
        with open(args.geo_location_rules,'r') as geo_location_rules_fh :
            for line in geo_location_rules_fh:
                geoRules.readFromLine( line )


    with open(args.genbank_data, "r") as genbank_fh :

        pipeline = (
            LineToJsonDataSource(genbank_fh)
            | RenameAndAddColumns(column_map = { 'collected': 'date',
                                                 'submitted': 'date_submitted'})
            | StandardizeData()
            | SequenceLengthFilter(15000)
        )

        if not args.sorted_fasta:
            pipeline = pipeline | DropSequenceData()

        pipeline = ( pipeline | StandardizeGenbankStrainNames()
                              | ParseGeographicColumnsGenbank( base / 'source-data/us-state-codes.tsv' )
                              | AbbreviateAuthors()
                              | AddHardcodedMetadataGenbank()
                              | ApplyUserGeoLocationSubstitutionRules(geoRules)
                              | MergeUserAnnotatedMetadata(annotations, idKey = 'genbank_accession' )
                              | MergeUserAnnotatedMetadata(accessions, idKey = 'genbank_accession_rev' )
                              | FillDefaultLocationData()
                              | GenbankProblematicFilter( args.problem_data,
                                                          ['genbank_accession', 'strain', 'region', 'country', 'url'],
                                                          restval = '?' , 
                                                          extrasaction ='ignore' , 
                                                          delimiter  = '\t', 
                                                          dict_writer_kwargs  = {'lineterminator': args.newline} )
        )

        sorted_metadata = sorted(
            pipeline,
            key=lambda obj: (
                obj['strain'],
                -obj['length'],
                obj['genbank_accession'],
                obj[LINE_NUMBER_KEY]
            )
        )
        #print( type( sorted_metadata ) )
        #print( sorted_metadata )

    # this should be moved further down
    # dedup by strain and compile a list of relevant line numbers.
    seen_strains = set()
    line_numbers = set()
    updated_strain_names_by_line_no = {}
    for entry in sorted_metadata:

        if entry['strain'] in seen_strains:
            continue

        seen_strains.add(entry['strain'])
        line_numbers.add(entry[LINE_NUMBER_KEY])
        updated_strain_names_by_line_no[entry[LINE_NUMBER_KEY]] = entry['strain']


    with open( args.output_metadata , 'wt' ) as metadata_OUT:
        dict_writer_kwargs = {'lineterminator': args.newline} 

        metadata_csv = csv.DictWriter(
            metadata_OUT,
            METADATA_COLUMNS,
            restval="",
            extrasaction='ignore',
            delimiter='\t',
            **dict_writer_kwargs
        )
        metadata_csv.writeheader()

        for entry in sorted_metadata :
            if entry[LINE_NUMBER_KEY] in line_numbers:
                metadata_csv.writerow(entry)
    
    if args.sorted_fasta:
        with open( args.output_fasta , 'wt' ) as fasta_OUT:
            for entry in sorted_metadata :
                if entry[LINE_NUMBER_KEY] in line_numbers:
                    strain_name = updated_strain_names_by_line_no[entry[LINE_NUMBER_KEY]]
                    print( '>' , strain_name , sep='' , file= fasta_OUT)
                    print( entry['sequence'] , file= fasta_OUT)                



    if not args.sorted_fasta:
        
        with open(args.genbank_data, "r") as genbank_IN , open(args.output_fasta, "wt", newline=args.newline) as fasta_OUT:
                for entry in (
                        LineToJsonDataSource(genbank_IN)
                        | RenameAndAddColumns(column_map = { 'collected': 'date',
                                                 'submitted': 'date_submitted'})
                        | StandardizeData()
                        | LineNumberFilter(line_numbers)
                ):
                    strain_name = updated_strain_names_by_line_no[entry[LINE_NUMBER_KEY]]
                    print( '>' , strain_name , sep='' , file= fasta_OUT)
                    print( entry['sequence'] , file= fasta_OUT)
                    
